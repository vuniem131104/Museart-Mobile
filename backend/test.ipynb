{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cd678b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b82ab0bf8204452d80aa7ad021babe1c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1854 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_24192/89309463.py:18: DeprecationWarning: invalid escape sequence '\\&'\n",
      "  decoded = raw_html.encode('utf-8').decode('unicode_escape')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to retrieve data for page 156\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1854"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "import html\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from langchain_core.documents.base import Document\n",
    "total_pages = 1854 \n",
    "import os \n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "def call_api(page):\n",
    "    url = f\"https://api.artic.edu/api/v1/products?page={page}\"\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        return response.json()['data']\n",
    "    else:\n",
    "        return None    \n",
    "    \n",
    "def html_to_text(raw_html):\n",
    "    if raw_html:\n",
    "        decoded = raw_html.encode('utf-8').decode('unicode_escape')\n",
    "        soup = BeautifulSoup(decoded, 'html.parser')\n",
    "        text = soup.get_text(separator=' ')\n",
    "        clean_text = ' '.join(text.split())\n",
    "        return clean_text\n",
    "    else:\n",
    "        return ''\n",
    "\n",
    "\n",
    "def generate_information(item):\n",
    "    product_title = item.get('title', 'Untitled')\n",
    "    raw_description = item.get('description', '')\n",
    "    description = html_to_text(raw_description)\n",
    "\n",
    "    decoded_price_html = html.unescape(item.get('price_display', ''))\n",
    "    match = re.search(r'\\$(\\d+(?:\\.\\d{1,2})?)', decoded_price_html)\n",
    "    if match:\n",
    "        price = float(match.group(1))\n",
    "    else:\n",
    "        price = 20.0  \n",
    "\n",
    "    image_url = item.get('image_url', '')\n",
    "\n",
    "    summary = (\n",
    "        f\"{product_title} is a beautifully crafted poster derived from the artwork featured in the exhibition. \"\n",
    "        f\"It has the description as: {description}. \"\n",
    "        f\"The piece reflects the powerful legacy of Frida Kahlo and Mary Reynolds as showcased in the Art Institute of Chicago. \"\n",
    "        f\"Priced at ${price:.2f}, it makes a unique addition to any art lover’s collection.\"\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        'product_title': product_title,\n",
    "        'price': price,\n",
    "        'image': image_url,\n",
    "        'summary': summary\n",
    "    }\n",
    "documents = []\n",
    "from tqdm.auto import tqdm \n",
    "for page in tqdm(range(1, total_pages + 1)):\n",
    "    data = call_api(page)\n",
    "    if data:\n",
    "        for item in data:\n",
    "            information = generate_information(item)\n",
    "            document = Document(page_content=information['summary'], metadata={\n",
    "                'product_title': information['product_title'],\n",
    "                'price': information['price'],\n",
    "                'image': information['image'],\n",
    "            })\n",
    "            documents.append(document)  \n",
    "    else:\n",
    "        print(f\"Failed to retrieve data for page {page}\")\n",
    "        break\n",
    "    \n",
    "len(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "31ce9871",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "from langchain_core.chat_history import BaseChatMessageHistory\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "from langchain.chains import create_retrieval_chain, create_history_aware_retriever\n",
    "from langchain_chroma import Chroma\n",
    "def create_vector_embeddings():\n",
    "    all_docs = documents\n",
    "    \n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=50)\n",
    "    \n",
    "    final_docs = text_splitter.split_documents(all_docs)\n",
    "    \n",
    "    embedding_model = OllamaEmbeddings(model=\"mxbai-embed-large\")\n",
    "    \n",
    "    print(\"Loaded Embedding Model\")\n",
    "    \n",
    "    embeddings = Chroma.from_documents(final_docs, \n",
    "                                       embedding_model, \n",
    "                                       collection_name=\"art_collection\",\n",
    "                                       persist_directory=\"./art_products\")\n",
    "    print(\"Loaded Chroma\")\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "566f8935",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded Embedding Model\n",
      "Loaded Chroma\n",
      "AI Response:  The Festive Tree Flat Decorative Candle is priced at $20.00, which is less than $40.00, and can be used for decoration. It's a handmade flat candle made in Lithuania with high-quality materials. It makes a unique addition to any art lover's collection.\n",
      "History:  [HumanMessage(content='Hey, i want some product having price less than 40.0 and they can be used for decoration purpose. Can you help me with that?', additional_kwargs={}, response_metadata={}), AIMessage(content=\"The Festive Tree Flat Decorative Candle is priced at $20.00, which is less than $40.00, and can be used for decoration. It's a handmade flat candle made in Lithuania with high-quality materials. It makes a unique addition to any art lover's collection.\", additional_kwargs={}, response_metadata={})]\n"
     ]
    }
   ],
   "source": [
    "llm = ChatGroq(groq_api_key=os.getenv(\"GROQ_API_KEY\"), model_name=\"meta-llama/llama-4-scout-17b-16e-instruct\")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "    Answer the questions based on the provided context only.\n",
    "    Please provide the most accurate respone based on the question\n",
    "    <context>\n",
    "    {context}\n",
    "    <context>\n",
    "    Question:{input}\n",
    "\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "embeddings = create_vector_embeddings()\n",
    "\n",
    "retriever = embeddings.as_retriever()\n",
    "\n",
    "document_chain = create_stuff_documents_chain(llm, prompt)\n",
    "\n",
    "contextualize_q_system_prompt=(\n",
    "            \"Given a chat history and the latest user question\"\n",
    "            \"which might reference context in the chat history, \"\n",
    "            \"formulate a standalone question which can be understood \"\n",
    "            \"without the chat history. Do NOT answer the question, \"\n",
    "            \"just reformulate it if needed and otherwise return it as is.\"\n",
    "        )\n",
    "\n",
    "contextualize_q_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", contextualize_q_system_prompt),\n",
    "        MessagesPlaceholder(\"chat_history\"),\n",
    "        (\"human\", \"{input}\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "history_aware_retriever = create_history_aware_retriever(llm, retriever, contextualize_q_prompt)\n",
    "\n",
    "system_prompt = (\n",
    "                \"You are an assistant for question-answering tasks. \"\n",
    "                \"Use the following pieces of retrieved context to answer \"\n",
    "                \"the question. If you don't know the answer, say that you \"\n",
    "                \"don't know. Use three sentences maximum and keep the \"\n",
    "                \"answer concise.\"\n",
    "                \"\\n\\n\"\n",
    "                \"{context}\"\n",
    "            )\n",
    "qa_prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\"system\", system_prompt),\n",
    "            MessagesPlaceholder(\"chat_history\"),\n",
    "            (\"human\", \"{input}\"),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "question_chain = create_stuff_documents_chain(llm, qa_prompt)\n",
    "\n",
    "rag_chain = create_retrieval_chain(history_aware_retriever, question_chain)\n",
    "\n",
    "store = {}\n",
    "\n",
    "def get_session_history(session_id:str)->BaseChatMessageHistory:\n",
    "    if session_id not in store:\n",
    "        store[session_id]=ChatMessageHistory()\n",
    "    return store[session_id]\n",
    "    \n",
    "conversational_rag_chain=RunnableWithMessageHistory(\n",
    "    rag_chain,get_session_history,\n",
    "    input_messages_key=\"input\",\n",
    "    history_messages_key=\"chat_history\",\n",
    "    output_messages_key=\"answer\"\n",
    ")\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     while True:\n",
    "input_text = \"Hey, i want some product having price less than 40.0 and they can be used for decoration purpose. Can you help me with that?\"\n",
    "\n",
    "session_history = get_session_history(\"default\")\n",
    "answer = conversational_rag_chain.invoke({\n",
    "    \"input\": input_text\n",
    "}, config={\n",
    "    \"configurable\": {\"session_id\": \"default\"}\n",
    "})\n",
    "print(\"AI Response: \", answer[\"answer\"])\n",
    "print(\"History: \", session_history.messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "d431e37a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI Response:  I can describe the product to you, but I'm a text-based AI and do not have the capability to provide direct links to products.\n",
      "\n",
      "The Festive Tree Flat Decorative Candle is a handmade flat candle made in Lithuania using high-quality German materials. It is dripless, smokeless, and self-extinguishing, and comes with an easy-to-assemble steel base. The dimensions of the candle are 2.4 x 0.4 x 5.9 inches and it is priced at $20.00.\n",
      "\n",
      "To find the product, I suggest searching online for \"Festive Tree Flat Decorative Candle\" or checking an online marketplace or store that sells it, such as the Art Institute of Chicago's website or an art store.\n"
     ]
    }
   ],
   "source": [
    "input_text = \"Describe the product in detail and provide the link to the product. I want to know about its price as well.\"\n",
    "\n",
    "session_history = get_session_history(\"default\")\n",
    "answer = conversational_rag_chain.invoke({\n",
    "    \"input\": input_text\n",
    "}, config={\n",
    "    \"configurable\": {\"session_id\": \"default\"}\n",
    "})\n",
    "print(\"AI Response: \", answer[\"answer\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f4012f26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.10.21'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pydantic \n",
    "\n",
    "pydantic.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ed07a5b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<groq.Groq at 0x7fc185438040>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from groq import Groq\n",
    "\n",
    "client = Groq(api_key='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')\n",
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "57e0e3ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Hello! I'm not a bro, but I'm happy to chat with you! How's your day going so far?\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = client.chat.completions.create(\n",
    "    model=\"llama3-70b-8192\",\n",
    "    messages=[{\"role\": \"user\", \"content\": \"Hello bro?\"}]\n",
    ")\n",
    "response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "635ed858",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatGroq(client=<groq.resources.chat.completions.Completions object at 0x7fc1850f7280>, async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x7fc185110a00>, model_name='meta-llama/llama-4-scout-17b-16e-instruct', model_kwargs={}, groq_api_key=SecretStr('**********'))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "llm = ChatGroq(groq_api_key='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', model_name=\"meta-llama/llama-4-scout-17b-16e-instruct\")\n",
    "llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7e9fde03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected exception formatting exception. Falling back to standard exception\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/vuiem/.local/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3526, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/tmp/ipykernel_67379/2843032526.py\", line 10, in <module>\n",
      "    response = chat.invoke(\"Tell me a joke about cats!\")\n",
      "  File \"/home/vuiem/.local/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\", line 368, in invoke\n",
      "    self.generate_prompt(\n",
      "  File \"/home/vuiem/.local/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\", line 937, in generate_prompt\n",
      "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
      "  File \"/home/vuiem/.local/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\", line 759, in generate\n",
      "    self._generate_with_cache(\n",
      "  File \"/home/vuiem/.local/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\", line 1002, in _generate_with_cache\n",
      "    result = self._generate(\n",
      "  File \"/home/vuiem/.local/lib/python3.10/site-packages/langchain_groq/chat_models.py\", line 480, in _generate\n",
      "    response = self.client.create(messages=message_dicts, **params)\n",
      "  File \"/home/vuiem/.local/lib/python3.10/site-packages/groq/resources/chat/completions.py\", line 322, in create\n",
      "    return self._post(\n",
      "  File \"/home/vuiem/.local/lib/python3.10/site-packages/groq/_base_client.py\", line 1225, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "  File \"/home/vuiem/.local/lib/python3.10/site-packages/groq/_base_client.py\", line 917, in request\n",
      "    return self._request(\n",
      "  File \"/home/vuiem/.local/lib/python3.10/site-packages/groq/_base_client.py\", line 953, in _request\n",
      "    response = self._client.send(\n",
      "  File \"/home/vuiem/.local/lib/python3.10/site-packages/httpx/_client.py\", line 914, in send\n",
      "    response = self._send_handling_auth(\n",
      "  File \"/home/vuiem/.local/lib/python3.10/site-packages/httpx/_client.py\", line 942, in _send_handling_auth\n",
      "    response = self._send_handling_redirects(\n",
      "  File \"/home/vuiem/.local/lib/python3.10/site-packages/httpx/_client.py\", line 979, in _send_handling_redirects\n",
      "    response = self._send_single_request(request)\n",
      "  File \"/home/vuiem/.local/lib/python3.10/site-packages/httpx/_client.py\", line 1015, in _send_single_request\n",
      "    response = transport.handle_request(request)\n",
      "  File \"/home/vuiem/.local/lib/python3.10/site-packages/httpx/_transports/default.py\", line 233, in handle_request\n",
      "    resp = self._pool.handle_request(req)\n",
      "  File \"/home/vuiem/.local/lib/python3.10/site-packages/httpcore/_sync/connection_pool.py\", line 256, in handle_request\n",
      "    raise exc from None\n",
      "  File \"/home/vuiem/.local/lib/python3.10/site-packages/httpcore/_sync/connection_pool.py\", line 236, in handle_request\n",
      "    response = connection.handle_request(\n",
      "  File \"/home/vuiem/.local/lib/python3.10/site-packages/httpcore/_sync/connection.py\", line 101, in handle_request\n",
      "    raise exc\n",
      "  File \"/home/vuiem/.local/lib/python3.10/site-packages/httpcore/_sync/connection.py\", line 78, in handle_request\n",
      "    stream = self._connect(request)\n",
      "  File \"/home/vuiem/.local/lib/python3.10/site-packages/httpcore/_sync/connection.py\", line 124, in _connect\n",
      "    stream = self._network_backend.connect_tcp(**kwargs)\n",
      "  File \"/home/vuiem/.local/lib/python3.10/site-packages/httpcore/_backends/sync.py\", line 208, in connect_tcp\n",
      "    sock = socket.create_connection(\n",
      "  File \"/usr/lib/python3.10/socket.py\", line 833, in create_connection\n",
      "    sock.connect(sa)\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/vuiem/.local/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 2120, in showtraceback\n",
      "    stb = self.InteractiveTB.structured_traceback(\n",
      "  File \"/home/vuiem/.local/lib/python3.10/site-packages/IPython/core/ultratb.py\", line 1435, in structured_traceback\n",
      "    return FormattedTB.structured_traceback(\n",
      "  File \"/home/vuiem/.local/lib/python3.10/site-packages/IPython/core/ultratb.py\", line 1326, in structured_traceback\n",
      "    return VerboseTB.structured_traceback(\n",
      "  File \"/home/vuiem/.local/lib/python3.10/site-packages/IPython/core/ultratb.py\", line 1173, in structured_traceback\n",
      "    formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n",
      "  File \"/home/vuiem/.local/lib/python3.10/site-packages/IPython/core/ultratb.py\", line 1088, in format_exception_as_a_whole\n",
      "    frames.append(self.format_record(record))\n",
      "  File \"/home/vuiem/.local/lib/python3.10/site-packages/IPython/core/ultratb.py\", line 970, in format_record\n",
      "    frame_info.lines, Colors, self.has_colors, lvals\n",
      "  File \"/home/vuiem/.local/lib/python3.10/site-packages/IPython/core/ultratb.py\", line 792, in lines\n",
      "    return self._sd.lines\n",
      "  File \"/home/vuiem/.local/lib/python3.10/site-packages/stack_data/utils.py\", line 145, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "  File \"/home/vuiem/.local/lib/python3.10/site-packages/stack_data/core.py\", line 734, in lines\n",
      "    pieces = self.included_pieces\n",
      "  File \"/home/vuiem/.local/lib/python3.10/site-packages/stack_data/utils.py\", line 145, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "  File \"/home/vuiem/.local/lib/python3.10/site-packages/stack_data/core.py\", line 677, in included_pieces\n",
      "    scope_pieces = self.scope_pieces\n",
      "  File \"/home/vuiem/.local/lib/python3.10/site-packages/stack_data/utils.py\", line 145, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "  File \"/home/vuiem/.local/lib/python3.10/site-packages/stack_data/core.py\", line 614, in scope_pieces\n",
      "    scope_start, scope_end = self.source.line_range(self.scope)\n",
      "  File \"/home/vuiem/.local/lib/python3.10/site-packages/stack_data/core.py\", line 178, in line_range\n",
      "    return line_range(self.asttext(), node)\n",
      "AttributeError: 'Source' object has no attribute 'asttext'\n"
     ]
    }
   ],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "\n",
    "# Khởi tạo model\n",
    "chat = ChatGroq(\n",
    "    model=\"llama3-70b-8192\",  # hoặc llama3-70b-8192\n",
    "    groq_api_key=\"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\",  # thay bằng API key của bạn\n",
    ")\n",
    "\n",
    "# Gửi prompt và nhận kết quả\n",
    "response = chat.invoke(\"Tell me a joke about cats!\")\n",
    "\n",
    "print(response.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "83b1e5c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected exception formatting exception. Falling back to standard exception\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/vuiem/.local/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3526, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/tmp/ipykernel_67379/1058789814.py\", line 1, in <module>\n",
      "    llm.invoke(\"Hello\")\n",
      "  File \"/home/vuiem/.local/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\", line 368, in invoke\n",
      "    self.generate_prompt(\n",
      "  File \"/home/vuiem/.local/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\", line 937, in generate_prompt\n",
      "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
      "  File \"/home/vuiem/.local/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\", line 759, in generate\n",
      "    self._generate_with_cache(\n",
      "  File \"/home/vuiem/.local/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\", line 1002, in _generate_with_cache\n",
      "    result = self._generate(\n",
      "  File \"/home/vuiem/.local/lib/python3.10/site-packages/langchain_groq/chat_models.py\", line 480, in _generate\n",
      "    response = self.client.create(messages=message_dicts, **params)\n",
      "  File \"/home/vuiem/.local/lib/python3.10/site-packages/groq/resources/chat/completions.py\", line 322, in create\n",
      "    return self._post(\n",
      "  File \"/home/vuiem/.local/lib/python3.10/site-packages/groq/_base_client.py\", line 1225, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "  File \"/home/vuiem/.local/lib/python3.10/site-packages/groq/_base_client.py\", line 917, in request\n",
      "    return self._request(\n",
      "  File \"/home/vuiem/.local/lib/python3.10/site-packages/groq/_base_client.py\", line 953, in _request\n",
      "    response = self._client.send(\n",
      "  File \"/home/vuiem/.local/lib/python3.10/site-packages/httpx/_client.py\", line 914, in send\n",
      "    response = self._send_handling_auth(\n",
      "  File \"/home/vuiem/.local/lib/python3.10/site-packages/httpx/_client.py\", line 942, in _send_handling_auth\n",
      "    response = self._send_handling_redirects(\n",
      "  File \"/home/vuiem/.local/lib/python3.10/site-packages/httpx/_client.py\", line 979, in _send_handling_redirects\n",
      "    response = self._send_single_request(request)\n",
      "  File \"/home/vuiem/.local/lib/python3.10/site-packages/httpx/_client.py\", line 1015, in _send_single_request\n",
      "    response = transport.handle_request(request)\n",
      "  File \"/home/vuiem/.local/lib/python3.10/site-packages/httpx/_transports/default.py\", line 233, in handle_request\n",
      "    resp = self._pool.handle_request(req)\n",
      "  File \"/home/vuiem/.local/lib/python3.10/site-packages/httpcore/_sync/connection_pool.py\", line 256, in handle_request\n",
      "    raise exc from None\n",
      "  File \"/home/vuiem/.local/lib/python3.10/site-packages/httpcore/_sync/connection_pool.py\", line 236, in handle_request\n",
      "    response = connection.handle_request(\n",
      "  File \"/home/vuiem/.local/lib/python3.10/site-packages/httpcore/_sync/connection.py\", line 101, in handle_request\n",
      "    raise exc\n",
      "  File \"/home/vuiem/.local/lib/python3.10/site-packages/httpcore/_sync/connection.py\", line 78, in handle_request\n",
      "    stream = self._connect(request)\n",
      "  File \"/home/vuiem/.local/lib/python3.10/site-packages/httpcore/_sync/connection.py\", line 124, in _connect\n",
      "    stream = self._network_backend.connect_tcp(**kwargs)\n",
      "  File \"/home/vuiem/.local/lib/python3.10/site-packages/httpcore/_backends/sync.py\", line 208, in connect_tcp\n",
      "    sock = socket.create_connection(\n",
      "  File \"/usr/lib/python3.10/socket.py\", line 833, in create_connection\n",
      "    sock.connect(sa)\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/vuiem/.local/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 2120, in showtraceback\n",
      "    stb = self.InteractiveTB.structured_traceback(\n",
      "  File \"/home/vuiem/.local/lib/python3.10/site-packages/IPython/core/ultratb.py\", line 1435, in structured_traceback\n",
      "    return FormattedTB.structured_traceback(\n",
      "  File \"/home/vuiem/.local/lib/python3.10/site-packages/IPython/core/ultratb.py\", line 1326, in structured_traceback\n",
      "    return VerboseTB.structured_traceback(\n",
      "  File \"/home/vuiem/.local/lib/python3.10/site-packages/IPython/core/ultratb.py\", line 1173, in structured_traceback\n",
      "    formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n",
      "  File \"/home/vuiem/.local/lib/python3.10/site-packages/IPython/core/ultratb.py\", line 1088, in format_exception_as_a_whole\n",
      "    frames.append(self.format_record(record))\n",
      "  File \"/home/vuiem/.local/lib/python3.10/site-packages/IPython/core/ultratb.py\", line 970, in format_record\n",
      "    frame_info.lines, Colors, self.has_colors, lvals\n",
      "  File \"/home/vuiem/.local/lib/python3.10/site-packages/IPython/core/ultratb.py\", line 792, in lines\n",
      "    return self._sd.lines\n",
      "  File \"/home/vuiem/.local/lib/python3.10/site-packages/stack_data/utils.py\", line 145, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "  File \"/home/vuiem/.local/lib/python3.10/site-packages/stack_data/core.py\", line 734, in lines\n",
      "    pieces = self.included_pieces\n",
      "  File \"/home/vuiem/.local/lib/python3.10/site-packages/stack_data/utils.py\", line 145, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "  File \"/home/vuiem/.local/lib/python3.10/site-packages/stack_data/core.py\", line 677, in included_pieces\n",
      "    scope_pieces = self.scope_pieces\n",
      "  File \"/home/vuiem/.local/lib/python3.10/site-packages/stack_data/utils.py\", line 145, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "  File \"/home/vuiem/.local/lib/python3.10/site-packages/stack_data/core.py\", line 614, in scope_pieces\n",
      "    scope_start, scope_end = self.source.line_range(self.scope)\n",
      "  File \"/home/vuiem/.local/lib/python3.10/site-packages/stack_data/core.py\", line 178, in line_range\n",
      "    return line_range(self.asttext(), node)\n",
      "AttributeError: 'Source' object has no attribute 'asttext'\n"
     ]
    }
   ],
   "source": [
    "llm.invoke(\"Hello\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
